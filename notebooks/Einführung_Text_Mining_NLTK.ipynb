{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in Text Mining mit dem Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen, auf denen dieser Workshop aufbaut\n",
    "\n",
    "Der vorliegende Workshop basiert zu einem Großteil auf: Alex, Beatrice and Llewellyn, Clare. (2020) Library Carpentry: Text & Data Mining. Centre for Data, Culture & Society, University of Edinburgh. http://librarycarpentry.org/lc-tdm/. Licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) 2016–2020 by [Library Carpentry](https://librarycarpentry.org/). Die konkret übernommenen Inhalte sowie Abweichungen von der Vorlage werden unter \"Details\" genauer beschrieben. Der vorliegende Workshop nutzt die Inhalte der Vorlage im Rahmen der CC-BY-4.0-Lizenz, wurde aber ohne Beteiligung der Urheber der Vorlage erstellt und auch nicht von diesen geprüft oder aktiv unterstützt.\n",
    "<details>\n",
    "Aus der o. g. Vorlage wurde der grundsätzliche Aufbau des Workshops beginnend mit einer kurzen Einführung in das Thema Text Mining, die Bedienung von Jupyter Notebooks, Grundlagen in Python und die Demonstration exemplarischer NLTK-Funktionen übernommen. Konkret übernommene Inhalte werden unten genauer beschrieben. Alle Textinhalte, die aus der Vorlage übernommen wurden, wurden vom Englischen ins Deutsche übersetzt und häufig sprachlich umformuliert. \n",
    "<br/><br/>     \n",
    "Die Abschnitte \n",
    "<ul>\n",
    "    <li>\"Was ist Text Mining und wofür kann es eingesetzt werden?\"</li>\n",
    "    <li>\"Datenzusammenstellung: Wie komme ich an geeignetes wissenschaftliches Textmaterial?\"</li>\n",
    "</ul>\n",
    "wurden nicht aus der Vorlage übernommen. Die Quellen der Inhalte werden in den Abschnitten selbst angegeben.\n",
    "<br/><br/>    \n",
    "Als Textmaterial wird im vorliegenden Workshop der CORD-19-Datensatz verwendet statt eines Ausschnitts aus der Medical History of British India collection wie in der Vorlage. \n",
    "<br/><br/>    \n",
    "Der Abschnitt \"Bedienung Jupyter Notebook\" inkl. Screenshots wurde relativ unabhängig von der Vorlage erstellt. \n",
    "<br/><br/> \n",
    "Beim Abschnitt \"Python\" wurde sich inhaltlich stark am Aufbau der Vorlage orientiert, die verwendeten Beispiele und Erklärungstexte aber abgeändert. Dies betrifft die Unterabschnitte\n",
    "<ul>\n",
    "    <li>\"Print-Funktion\"</li>\n",
    "    <li>\"Variablen\"</li>\n",
    "    <li>\"Einige wichtige Datentypen\"</li>\n",
    "    <li>\"Datentyp Liste\"</li>\n",
    "    <li>\"for Schleife\"</li>\n",
    "    <li>\"if/elif/else-Ausdrücke\"</li>\n",
    "    <li>\"Datentyp Dictionary\"</li>\n",
    "</ul>\n",
    "    \n",
    "Die folgenden Unterabschnitte wurden neu hinzugefügt:\n",
    "<ul>\n",
    "    <li>\"list comprehension\"</li>\n",
    "    <li>\"len() Funktion zur Ausgabe der Anzahl der Elemente einer Liste\"</li>\n",
    "</ul>\n",
    "    \n",
    "Da der vorliegende Workshop im Gegensatz zur Vorlage den CORD-19-Datensatz als Datengrundlage für die Text-Mining-Analysen verwendet, wurde der Abschnitt \"Datenzusammenstellung aus dem CORD-19-Korpus\" neu erstellt und basiert nicht auf der Vorlage.\n",
    "    \n",
    "Bei den Abschnitten, die sich mit konkreten NLTK-Funktionen beschäftigen, d. h. alle Abschnitte ab \"Python-Bibliotheken für die folgenden Analysen\", orientiert sich der vorliegende Workshop sehr eng an der Vorlage. Insbesondere wurden die Codebeispiele aus der Vorlage übernommen, wenn auch mit teilweise leicht abgeänderten Variablenbezeichnungen (z. B. <code>lower_cord_tokens</code> statt <code>lower_india_tokens</code>).\n",
    "</details>\n",
    "\n",
    "Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python\n",
    "– Analyzing Text with the Natural Language Toolkit. http://www.nltk.org/book/\n",
    "\n",
    "COVID-19 Open Research Dataset (CORD-19), https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge. Im Rahmen des Workshops werden Abstracts unter CC-BY-Lizenzen aus der metadata.csv-Datei des Datensatzes (Stand: 10.03.2021) ausgewertet.\n",
    "\n",
    "Weitere verwendete Literatur (wird im Jupyter Notebook an den entsprechenden Stellen zitiert):\n",
    "\n",
    "[1] Drees, B. (2016). Text und Data Mining: Herausforderungen und Möglichkeiten für Bibliotheken. Perspektive Bibliothek, 5(1), 49–73. https://doi.org/10.11588/pb.2016.1.33691\n",
    "\n",
    "[2] What Is Text Mining? A Beginner's Guide. https://monkeylearn.com/text-mining/\n",
    "\n",
    "[3] Rakesh Agrawal, Tomasz Imieliński, and Arun Swami. 1993. Mining association rules between sets of items in large databases. SIGMOD Rec. 22, 2 (June 1, 1993), 207–216. DOI:https://doi.org/10.1145/170036.170072\n",
    "\n",
    "[4] A Friendly Introduction to Text Clustering. https://towardsdatascience.com/a-friendly-introduction-to-text-clustering-fa996bcefd04 \n",
    "\n",
    "[5] Weeber M, Vos R, Klein H, De Jong-Van Den Berg LT, Aronson AR, Molema G. Generating hypotheses by discovering implicit associations in the literature: a case report of a search for new potential therapeutic uses for thalidomide. J Am Med Inform Assoc. 2003 May-Jun;10(3):252-9. doi: 10.1197/jamia.M1158. Epub 2003 Jan 28. PMID: 12626374; PMCID: PMC342048.\n",
    "\n",
    "[6] Perkins, J. (2014) Python 3 Text Processing with NLTK 3 Cookbook, ISBN 9781782167853 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhalte des Workshops\n",
    "\n",
    "* Theorie\n",
    "  * Was ist Text Mining und wofür kann es eingesetzt werden?\n",
    "      * Mögliche Definition und Ablauf\n",
    "      * Text-Mining-Methoden\n",
    "      * Anwendungsbeispiel\n",
    "      * Datenzusammenstellung: Wie komme ich an geeignetes wissenschaftliches Textmaterial?\n",
    "* Praxis: Untersuchung von Abstracts aus dem CORD-19-Datensatz\n",
    "  * Bedienung Jupyter-Notebook\n",
    "  * Einführung in Python\n",
    "    * Vorteile\n",
    "    * Print-Funktion\n",
    "    * Variablen\n",
    "    * Einige wichtige Datentypen\n",
    "    * Ausgabe des Typs einer Variable mit der Funktion type()\n",
    "    * Datentyp Liste\n",
    "    * for-Schleifen\n",
    "      * list comprehensions\n",
    "    * if/elif/else-Ausdrücke\n",
    "    * Datentyp Dictionary\n",
    "  * Ziele der Analyse des CORD-19-Datensatzes\n",
    "  * Datenzusammenstellung aus dem CORD-19-Korpus\n",
    "  * Datenaufbereitung\n",
    "    * Extrahieren der Abstracts aus einer csv-Datei und Schreiben in eine Textdatei\n",
    "    * Tokenisierung\n",
    "    * Normalisierung (Umwandlung aller Tokens in Kleinschreibung)\n",
    "    * Stoppwortentfernung\n",
    "  * Analyse\n",
    "    * Konkordanzliste (Kontexte von Token)\n",
    "    * Häufigkeitsverteilung\n",
    "        * Diagramm\n",
    "        * Wortwolke\n",
    "    * Kollokationen (häufig zusammen auftretende Token)"
   ]
  },
  {
   "attachments": {
    "grafik-3.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theorie\n",
    "### Was ist Text Mining und wofür kann es eingesetzt werden?\n",
    "\n",
    "**Mögliche Definition und Ablauf**\n",
    "\n",
    "* Text Mining kann als die automatisierte, systematische, auf Algorithmen gestützte statistische Auswertung großer Datenmengen bezeichnet werden und\n",
    "* dient der Auffindung von (bisher unbekannten) Zusammenhängen, strukturellen Ähnlichkeiten sowie deren Nutzung zur Extrapolation (z.B. Vorhersage von Eigenschaften)\n",
    "![grafik-3.png](attachment:grafik-3.png) in Anlehnung an https://www.fosteropenscience.eu/node/2152 \n",
    "\n",
    "**Einfache Text-Mining-Methoden** [2]\n",
    "\n",
    "* Worthäufigkeiten\n",
    "* Konkordanzen (Kontexte, in denen ein Wort auftaucht)\n",
    "* Kollokationen (häufig zusammen auftretende Wörter)\n",
    "\n",
    "**Fortgeschrittene Text-Mining-Methoden** [1, 2]\n",
    "\n",
    "* Musterextraktion\n",
    "  * Assoziationsanalyse [3]\n",
    "    * Identifikation häufig zusammen auftretender Attribute\n",
    "    * häufige Anwendung: Warenkorbanalyse\n",
    "* Clustering [4]\n",
    "  * Gruppierung von ähnlichen Daten in Cluster\n",
    "  * \"unsupervised learning\": Cluster sind nicht vordefiniert\n",
    "  * Beispiele für Clustering-Algorithmen\n",
    "    * hierarchisches Clustering\n",
    "    * *k*-means-Clustering\n",
    "* Klassifikation\n",
    "  * \"supervised learning\": Klassen sind im Voraus bekannt\n",
    "  * Beispiele für Klassifikationsalgorithmen\n",
    "    * Nächste-Nachbarn-Klassifikation\n",
    "    * Entscheidungsbäume\n",
    "* Regressionsanalyse\n",
    "* Computerlinguistische Ansätze \n",
    "  * wichtiges Hilfsmittel: Ontologien\n",
    "  * Anwendung: z. B. Named Entity Recognition\n",
    "  \n",
    "**Anwendungsbeispiel: Suche nach Krankheiten, die potenziell mit Thalidomid behandelt werden können**\n",
    "\n",
    "Unter Nutzung des Swanson-ABC-Modells und des Unified Medical Language System (UMLS) Metathesaurus konnten Weeber et al. (2003) Krankheiten identifizieren, die möglicherweise mit Thalidomid behandelt werden können. [5]\n",
    "\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" src=\"Jupyter_Notebook_Screenshots/Anwendungsbeispiel_Text_Mining_Thalidomid.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenzusammenstellung: Wie komme ich an geeignetes wissenschaftliches Textmaterial?\n",
    "\n",
    "**Arten von Textmaterial**\n",
    "* Volltexte\n",
    "* Abstracts\n",
    "* Metadaten\n",
    "* ...\n",
    "\n",
    "**Datenformate**\n",
    "* Maschinenlesbar, strukturiert (z. B. XML, CSV) -> sehr gut für TDM-Anwendungen geeignet\n",
    "* Maschinenlesbar (z. B. PDF mit durchsuchbarem Volltext) -> für TDM-Anwendungen geeignet\n",
    "* Nicht maschinenlesbar (z. B. PDF, in denen die Seiten als Bilder vorliegen) -> ohne OCR nicht für TDM-Anwendungen geeignet\n",
    "\n",
    "**Wissenschaftliche Volltexte und Abstracts z. B. aus OA-Journals wie**\n",
    "  * SpringerOpen (Abruf von Artikeln per API im XML-JATS-Format)\n",
    "    * https://dev.springernature.com/\n",
    "  * Elsevier (Abruf von Artikeln per API in Elsevier-spezifischem Format)\n",
    "    * https://dev.elsevier.com/\n",
    "  * PlosOne (Bulk-Download aller Artikel im XML-JATS-Format möglich)\n",
    "    * https://plos.org/text-and-data-mining/\n",
    "  * ...\n",
    "  \n",
    "**Abstracts aus Datenbanken, z. B.**\n",
    "  * PubMed (Abruf von Abstracts per API)\n",
    "    * Artikel auf der Webseite: https://pubmed.ncbi.nlm.nih.gov/33540487/\n",
    "    * Abruf der Artikelmetadaten per API: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=33540487&retmode=xml&rettype=abstract\n",
    "  * SCOAP3 (Hochenergiephysik)\n",
    "    * https://github.com/SCOAP3/scoap3-next/wiki/API-documentation\n",
    "\n",
    "**Thematische, öffentlich verfügbare Korpora, z. B.**\n",
    "  * COVID-19 Open Research Dataset (CORD-19) \n",
    "    * https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n",
    "    * Dieses Datenset wird für den heutigen Workshop verwendet\n",
    "    \n",
    "**Unterstützung bei der Beschaffung von Literatur für TDM-Projekte**\n",
    "\n",
    "Die ULB Darmstadt unterstützt Sie dabei, Literatur für Text-und-Data-Mining-Anwendungen zu beschaffen. Falls Sie beispielsweise bestimmte Zeitschriften, die die ULB lizenziert, im Rahmen einer TDM-Analyse auswerten möchten, bieten wir an, mit den betreffenden Verlagen in Kontakt zu treten und die Nutzungsbedingungen zu klären (z. B. ob der Verlag die gewünschten Artikel, ggf. in strukturierten Formaten, bereitstellen würde oder ob ein automatisiertes Harvesten von Artikeln von der Verlagswebseite erlaubt ist). Bitte wenden Sie sich diesbezüglich an das Team Text und Data Mining der ULB (tdm@ulb.tu-darmstadt.de)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praxis: Untersuchung von Abstracts aus dem CORD-19-Datensatz\n",
    "\n",
    "### Bedienung Jupyter Notebook\n",
    "\n",
    "**Editor-Oberfläche**\n",
    "* Markdown-Zellen (Freitext)\n",
    "* Code-Zellen (Softwarecode + Output)\n",
    "\n",
    "Wechsel zwischen Zelltypen über die Menüleiste\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" src=\"Jupyter_Notebook_Screenshots/Jupyter_Change_Celltype.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>\n",
    "\n",
    "**Neue Zelle erstellen**\n",
    "\n",
    "* Eine bestehende Zelle mit einfachem Linksklick auswählen\n",
    "* Über das Plus-Symbol in der Menüleiste eine neue Zelle darunter einfügen\n",
    "* Über das Dropdown-Menü (s. o.) den Zelltyp auswählen\n",
    "\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" width=550 src=\"Jupyter_Notebook_Screenshots/Add_Cell_Below_v1.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Übung**\n",
    "\n",
    "Erzeugen Sie bitte unter dieser Zelle eine neue leere Markdown-Zelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zelle löschen**\n",
    "\n",
    "* Zelle mit einfachem Linksklick öffnen \n",
    "* im Menü Edit -> Delete Cells\n",
    "* Alternative: Nutzen des Ausschneiden-Symbols (Zelle befindet sich anschließend in der Zwischenablage)\n",
    "\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" width=550 src=\"Jupyter_Notebook_Screenshots/Cut_Cell_v1.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Übung**\n",
    "\n",
    "Löschen Sie bitte die gerade oben erzeugte Zelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bearbeitungsmodi**\n",
    "\n",
    "1\\. Command Mode (blauer Zellenrand)\n",
    "* Zellen können als Ganzes bearbeitet werden\n",
    "* einfacher Klick auf eine Zelle öffnet diese im Command Mode\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" src=\"Jupyter_Notebook_Screenshots/Jupyter_Cell_Command_Mode.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>\n",
    "\n",
    "2\\. Edit Mode (grüner Zellenrand) \n",
    "* Zellinhalt der einzelnen Zelle kann bearbeitet werden\n",
    "* Doppelklick auf eine Zelle öffnet diese im Edit Mode\n",
    "* Wechsel zurück in den Command Mode mit ESC\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" src=\"Jupyter_Notebook_Screenshots/Jupyter_Cell_Edit_Mode.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formatierung Freitext mit Markdown**\n",
    "\n",
    "Bei Markdown handelt es sich um eine Auszeichnungssprache, mit der sich Texte durch einfache Befehle formatieren lassen. Ein Cheatsheet mit wichtigen Markdown-Befehlen befindet sich unter https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet.\n",
    "\n",
    "Beispiele für Markdown-Auszeichnungen\n",
    "\n",
    "Kursivschrift  \n",
    "\\*Beispieltext\\* = *Beispieltext*  \n",
    "\n",
    "Fettschrift  \n",
    "\\*\\*Beispieltext\\*\\* = **Beispieltext** \n",
    "\n",
    "Überschriften  \n",
    "\\# Überschrift = **Überschrift** (1. Ebene)  \n",
    "\\## Überschrift = **Überschrift** (2. Ebene)  \n",
    "... \n",
    "\n",
    "Übersicht Markdownbefehle: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
    "\n",
    "Nachdem Text in eine Markdown-Zelle eingetragen worden ist, muss die Zelle mit dem Run-Button in der Menüleiste ausgeführt werden, damit die Formatierung wirksam wird.\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" width = 550 src=\"Jupyter_Notebook_Screenshots/Run_Cell.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aufgabe 1:*  \n",
    "* *Erzeugen Sie bitte unter dieser Zelle eine neue Markdown-Zelle.* \n",
    "* *Tragen Sie in diese Zelle eine Überschrift \"Markdown-Formatierung testen\" ein. Die Überschrift soll sich auf der dritten Ebene befinden.*\n",
    "* *Bitte geben Sie darunter in Fettschrift ein (ohne die Anführungszeichen): \"Dieser Text soll fett dargestellt werden.\"*\n",
    "\n",
    "<p>\n",
    "<details>\n",
    "1. Markieren Sie die aktuelle Zelle mit einem Klick in den linken weißen Rand. Die Zelle sollte danach einen blauen Rahmen haben. (Falls die Zelle einen grünen Rahmen hat, drücken Sie bitte einmal die Escape-Taste.)<br/>\n",
    "2. Klicken Sie danach in der Menüleiste auf das Plus-Symbol. Die neue Zelle ist zunächst vom Typ \"Code\", erkennbar an dem Zähler \"In [ ]:\" vor der Zelle.<br/>\n",
    "3. Bitte wählen Sie im Dropdown-Menü in der Menüleiste den Typ \"Markdown\" aus.<br/>\n",
    "    4. Tragen Sie in die Zelle den String <code>### Markdown-Formatierung testen</code> ein.<br/>\n",
    "5. Tragen Sie in die Zeile unter der Überschrift ein: <code>**Dieser Text soll fett dargestellt werden.**</code><br/>\n",
    "6. Bestätigen Sie mit der Schaltfläche \"Run\" in der Menüleiste\n",
    "</details>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "### Vorteile\n",
    "\n",
    "* große Vielfalt frei zugänglicher Softwarebibliotheken (heutiger Workshop: Natural Language Toolkit)\n",
    "* Python-Code kann in allen Betriebssystemen ausgeführt werden, auf denen der Python-Interpreter installiert werden kann\n",
    "* Python-Code kann direkt im Jupyter Notebook ausgeführt werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print-Funktion \n",
    "\n",
    "Um zu testen, dass Python-Code tatsächlich im Jupyter-Notebook ausgeführt werden kann, soll mittels der Funktion ```print()``` der String \"Die Python-Ausgabe funktioniert!\" ausgegeben werden.\n",
    "\n",
    "*Aufgabe: Erzeugen Sie in einer neuen Code-Zelle eine Python-Ausgabe mit dem String \"Die Python-Ausgabe funktioniert!\"*\n",
    "<p>\n",
    "<details>\n",
    "1. Erzeugen Sie wie in Aufgabe 1 beschrieben eine neue Zelle, belassen sie aber im Dropdown-Menü auf \"Code\" und wechseln nicht auf \"Markdown\".<br/>\n",
    "2. Doppelklicken Sie auf die Zelle und geben den folgenden Code ein: \n",
    "<code>print(\"Die Python-Ausgabe funktioniert!\")</code><br/>\n",
    "3. Führen Sie die Zelle mit dem Run-Button in der Menüleiste aus.\n",
    "</details>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variablen\n",
    "\n",
    "Variablen sind \"Container\", in denen Werte abgelegt und später wieder ausgelesen werden können. \n",
    "\n",
    "Vorlage: ```x = \"Die Python-Ausgabe funktioniert!\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgeben des Inhalts der Variable.\n",
    "\n",
    "Vorlage: ```x```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zuweisen und Ausgeben von Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variablennamen können weitgehend frei gewählt werden. (Einige wenige Regeln müssen eingehalten werden, z. B. darf der Name nicht mit einer Zahl beginnen und keine Leerzeichen enthalten):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorlage: `aussage = \"Die Python-Ausgabe funktioniert!\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgeben des Inhalts der Variable `aussage`\n",
    "\n",
    "Vorlage: ```aussage```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einige wichtige Datentypen\n",
    "* Integer (Ganzzahlen)\n",
    "* Float (Kommazahlen)\n",
    "* String (Zeichenketten)\n",
    "* List (Listen von Werten)\n",
    "* Dictionaries (Listen von Schlüssel-Wert-Paaren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgabe des Typs einer Variable mit der Funktion type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorlage: `type(aussage)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datentyp Liste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einer Liste können Daten in geordneter Form gespeichert werden, beispielsweise die Wörter und Satzzeichen eines Satzes. Im folgenden Beispiel wird der Satz \"Dies ist ein Beispielsatz.\" in einer Liste mit dem Namen ```satz``` gespeichert.\n",
    "\n",
    "Vorlage: `satz = ['Dies', 'ist', 'ein', 'Beispielsatz', '.']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe der Liste `satz`\n",
    "\n",
    "Vorlage: ```satz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedes Element einer Liste hat eine Indexnummer, entsprechend seiner Position in der Liste:\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" width=\"450\" src=\"Jupyter_Notebook_Screenshots/Nummerierung_Liste_Beispielsatz_v1.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>\n",
    "\n",
    "Ausgabe des zweiten Listenelements\n",
    "\n",
    "Vorlage: `satz[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for-Schleifen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel: Mit einer ```for```-Schleife sollen alle Elemente der Liste ```satz``` nacheinander ausgegeben werden.\n",
    "\n",
    "Die Syntax einer ```for```-Schleife hat die Form ```for x in y:``` Dabei ist ```y``` ein iterierbares Objekt, also ein Objekt, das mehrere Elemente enthält, beispielsweise eine Liste. Beim ersten Schleifendurchlauf wird der Variablen ```x``` das erste Element der Liste zugewiesen, beim zweiten Schleifendurchlauf das zweite usw. Sobald die Liste vollständig durchlaufen wurde, bricht die Schleife ab. \n",
    "\n",
    "Der Code unter der ersten Zeile gibt an, was jeweils mit der Variable getan werden soll. Der Code muss eingerückt werden (z. B. mit vier Leerzeichen oder einem Tab), damit Python erkennt, dass es sich um den Anweisungskörper der Schleife handelt. Im folgenden Beispiel sollen die Elemente der Liste ```satz``` mithilfe der ```print()``` Funktion ausgegeben werden.\n",
    "\n",
    "Vorlage: \n",
    "```python \n",
    "for element in satz:\n",
    "    print(element)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt werden wir eine ```for``` Schleife verwenden, um alle Wörter in einer Liste in Kleinschreibung umzuwandeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine *list comprehension* in Python ist eine Vorschrift, mit der in einfacher Form aus den Elementen einer bestehenden Liste eine neue Liste erzeugt werden kann. Dabei kann auf jedes Element der ursprünglichen Liste eine Verarbeitungsanweisung angewendet werden.\n",
    "\n",
    "Im folgenden Beispiel sollen alle Elemente der Liste ```satz``` in Kleinschreibung umgewandelt und in eine neue Liste ```satz_kleingeschrieben``` eingetragen werden.\n",
    "\n",
    "Die entsprechende Anweisung \n",
    "```python\n",
    "satz_kleingeschrieben = [element.lower() for element in satz]\n",
    "``` \n",
    "steht in eckigen Klammern und zeigt damit an, dass es sich bei ```satz_kleingeschrieben``` um eine Liste handelt. Der erste Teil ```element.lower()``` enthält eine Anweisung, in diesem Fall, dass auf die Variable ```element``` die Funktion ```lower()``` angewendet werden soll, die den Inhalt von ```element``` in Kleinschreibung umwandelt. Im zweiten Teil wird dann festgelegt, dass es sich bei ```element``` um die Inhalte der Liste ```satz``` handelt, die nacheinander durchlaufen werden.\n",
    "\n",
    "Vorlage: ```satz_kleingeschrieben = [element.lower() for element in satz]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe der neuen Liste\n",
    "\n",
    "Vorlage: ```satz_kleingeschrieben```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### len() Funktion zur Ausgabe der Anzahl der Elemente einer Liste\n",
    "\n",
    "Die ```len()``` Funktion gibt die Anzahl der Elemente einer Liste zurück\n",
    "\n",
    "Vorlage: ```len(satz_kleingeschrieben)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die ```len()```-Funktion wird im Folgenden zur Veranschaulichung der Funktionsweise von ```if/elif/else```-Ausdrücken verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if/elif/else-Ausdrücke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit ```if/elif/else```-Ausdrücken kann die Ausführung von Code an Bedingungen geknüpft werden. Wenn die in der ```if```-Anweisung formulierte Bedingung wahr ist, wird die der zugehörige Code im Anweisungskörper ausgeführt. Mit ```elif```-Bedingungen (\"else if\") können weitere Bedingungen formuliert werden, die vom Interpreter von oben nach unten durchlaufen werden. Wenn eine Bedingung wahr ist, wird der entsprechende Code ausgeführt. Die folgenden ```elif```-Ausdrücke sowie ggf. der else-Ausdruck am Ende werden dann nicht mehr ausgewertet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(satz) == 2:\n",
    "    print(\"Die Liste enthält 2 Elemente.\")\n",
    "elif len(satz) > 2:\n",
    "    print(\"Die Liste enthält mehr als 2 Elemente.\")\n",
    "else:\n",
    "    print(\"Die Liste enthält weniger als 2 Elemente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Workshop werden wir durch eine List Comprehension, die eine ```if```-Anweisung enthält, nicht sinntragende Wörter (z. B. \"the\", \"and\", \"under\") aus einer Wortliste entfernen. Solche Wörter werden auch als Stoppwörter bezeichnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datentyp Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Dictionaries können Schlüssel-Wert-Paare hinterlegt werden. Im Folgenden wird ein Dictionary mit dem Namen *universitäten_dict* mit den Key/Value-Paaren Universitätsname/Studierendenanzahl angelegt:\n",
    "\n",
    "\"TU Darmstadt\" : 25900,  \n",
    "\"GU Frankfurt\" : 45000,  \n",
    "\"JGU Mainz\" : 33000  \n",
    "\n",
    "Vorlage: `universitäten_dict = {\"TU Darmstadt\" : 25900, \"GU Frankfurt\" : 45000, \"JGU Mainz\" : 33000}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Über den Schlüssel (hier: Universitätsname) kann die Studierendenzahl ausgegeben werden. Exemplarische Ausgabe der Studierendenzahl der JGU Mainz:\n",
    "\n",
    "Vorlage: `universitäten_dict[\"JGU Mainz\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Workshop werden wir ein Dictionary verwenden, um Wörter (key) und ihre Häufigkeit (value) abzuspeichern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ziele der Analyse des CORD-19-Datensatzes\n",
    "\n",
    "* Visualisieren der häufigsten charakteristischen Begriffe in einer Untermenge der verfügbaren Abstracts\n",
    "* Erstellung einer Konkordanzliste für ein Token\n",
    "* Ermittlung von Kollokationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenzusammenstellung aus dem CORD-19-Korpus\n",
    "\n",
    "1. Datenstruktur ansehen unter https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n",
    "  * Gesamt-Datenset sehr groß (36 GB)\n",
    "  * um in annehmbarer Zeit zu Ergebnissen zu kommen, Konzentration auf die Abstracts in der Datei metadata.csv (ca. 720 MB, Download unter https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge?select=metadata.csv)\n",
    "  * Abstracts befinden sich in der Spalte \"abstract\"\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" src=\"Jupyter_Notebook_Screenshots/Cord-19-CSV-Preview.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>\n",
    "\n",
    "[Bild vergrößern](Jupyter_Notebook_Screenshots/Cord-19-CSV-Preview.PNG)\n",
    "  * Ziel: Eine Auswahl der verfügbaren Abstracts soll in eine Textdatei hintereinanderkopiert werden, um diese anschließend zu analysieren (z. B. Worthäufigkeiten zu bestimmen). Es soll nur eine Auswahl der Abstracts analyisert werden, da eine Analyse aller Abstracts während des Workshops zu lange dauern würde.\n",
    "\n",
    "2. Recherche, ob es für das Datenset schon Programme gibt\n",
    "  * ja, z. B. https://github.com/allenai/cord19\n",
    "  * Code benötigt die Python-Bibliothek csv\n",
    "  * Link zur Dokumentation der csv-Bibliothek: https://docs.python.org/3/library/csv.html\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wird exemplarisch gezeigt, wie aus der ```metadata.csv```-Datei Abstracts extrahiert und hintereinander in eine Textdatei geschrieben werden können. Für den Workshop wurde bereits eine Textdatei mit Abstracts, die unter einer CC-BY-Lizenz stehen, mit einem Zufallsverfahren erzeugt. Diese Datei (```abstracts_cord_19_random.txt```) wird im Rahmen des Workshops analysiert. Das Skript, mit dem die Abstracts ausgewählt worden sind (```cord-19.py```) sowie eine Liste mit den Metadaten der ausgewählten Abstracts (```metadata_selected_articles.csv```) befindet sich im Ordner \"Zusatzmaterial\" der Hessenbox-Freigabe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Einlesen der CORD-19-CSV-Datei**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. csv-Bibliothek importieren:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. csv-Datei öffnen\n",
    "\n",
    "```python\n",
    "csv_file_object = open('metadata.csv', encoding = \"UTF-8\")\n",
    "```\n",
    "\n",
    "```csv_file_object``` = Variable, in die die csv-Datei geschrieben werden soll, sodass sie als Dateiobjekt in Python zur Verfügung steht\n",
    "\n",
    "```'metadata.csv'``` = Name der csv-Datei (Die Datei befindet sich im gleichen Ordner wie das Jupyter-Notebook, ansonsten müsste hier der Dateipfad angegeben werden.)\n",
    "\n",
    "```'encoding = UTF-8'``` (Angabe der Zeichencodierung zur korrekten Ausgabe von Sonderzeichen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_object = open('metadata.csv', encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Die Python-Bibliothek csv stellt eine DictReader-Klasse zur Verfügung, der man die eingelesene csv-Datei übergeben kann: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.DictReader(csv_file_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Die Inhalte der csv-Datei liegen nun in der Variable ```reader``` vor. Diese kann mit einer ```for```-Schleife durchlaufen werden. Jeder Durchlauf liefert eine Zeile der csv-Datei in Form eines Dictionaries zurück.\n",
    "\n",
    "Exemplarisch kann mit der folgenden Schleife die erste Zeile (also das erste Dictionary) ausgegeben werden. (Die  Schleife bricht nach der ersten Zeile ab, da es beim Ausgeben aller Zeilen der CSV-Datei passieren kann, dass sich das Jupyter-Notebook aufhängt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in reader:\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Jede Zeile der csv-Datei ist demnach ein Dictionary. Die Spaltennamen sind dabei die Schlüssel und Spalteninhalte die Werte des Dictionaries. Daher kann auf das Abstract oben folgendermaßen zugegriffen werden: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row[\"abstract\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schreiben des Abstracts in eine Textdatei**\n",
    "\n",
    "6\\. Datei anlegen, in die das Abstract geschrieben werden soll:\n",
    "\n",
    "```python\n",
    "abstract_output_file = open(\"example_abstract_cord-19.txt\", \"w\", encoding=\"UTF-8\")\n",
    "```\n",
    "\n",
    "```\"example_abstract_cord-19.txt\"```: Name der Datei, die angelegt werden soll. Wird wie hier kein Pfad angegeben, wird die Datei in das aktuelle Verzeichnis geschrieben.\n",
    "\n",
    "```abstract_output_file``` = Variable, in die das Dateiobjekt geschrieben wird.\n",
    "\n",
    "```\"w\"``` Die Datei wird im \"write\"-Modus geöffnet, d. h. Inhalte können in die Datei geschrieben werden. Existiert die Datei bereits, werden die Inhalte überschrieben, ansonsten wird die Datei neu erstellt.\n",
    "\n",
    "```encoding=\"UTF-8\"``` Die Angabe der Zeichenkodierung ist wichtig, damit Sonderzeichen korrekt in die Datei geschrieben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_output_file = open(\"example_abstract_cord-19.txt\", \"w\", encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Abstract in die Datei schreiben. Rückgabewert ist die Länge des geschriebenen Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_output_file.write(\"{} \".format(row[\"abstract\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Datei schließen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zusammengefasster Ablauf, mit dem 10 Abstracts der CSV-Datei in eine Textdatei geschrieben werden können** \n",
    "\n",
    "(in Anlehnung an https://github.com/allenai/cord19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Textdatei erzeugen, in die die Abstracts aus der csv-Datei kopiert werden sollen\n",
    "abstract_output_file = open(\"10_abstracts_cord-19.txt\", \"w\", encoding=\"UTF-8\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# open the file\n",
    "f_in = open('metadata.csv', encoding=\"UTF-8\")\n",
    "\n",
    "reader = csv.DictReader(f_in)\n",
    "\n",
    "for row in reader:\n",
    "    # Kommentar 1: Zur Begrenzung auf 10 Abstracts\n",
    "    counter += 1\n",
    "    if counter == 11:\n",
    "        break\n",
    "    else:\n",
    "    # Kommentar 2: Wenn alle Abstracts ausgelesen werden sollen, kann der Code zwischen den Kommentaren 1 und 2 entfernt werden\n",
    "        # Metadatenstrings anhand Spaltenkopf auslesen\n",
    "        abstract = row['abstract']\n",
    "\n",
    "        # Abstracts in Datei schreiben\n",
    "        abstract_output_file.write(\"{} \".format(abstract))\n",
    "\n",
    "f_in.close()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Für die folgenden Analysen verwendete Datei**\n",
    "\n",
    "Für die folgende Analyse wird die Datei ```abstracts_cord_19_random.txt``` verwendet, bei der durch ein Zufallsverfahren insgesamt 9607 Abstracts, die jeweils unter einer CC-BY-Lizenz stehen, aus der CORD-19 ```metadata.csv```-Datei ausgewählt worden sind. Das Python-Skript ```cord-19.py``` zur Auswahl der Abstracts, eine Liste mit den Metadaten der Artikel, deren Abstracts ausgewählt worden sind (```metadata_selected_articles.csv```), sowie eine Datei mit einer Übersicht, wie viele der Artikel des Datensatzes insgesamt unter einer CC-BY-Lizenz stehen (```number_of_articles.txt```), befinden sich im Ordner \"Zusatzmaterial\" der Hessenbox-Freigabe. Die ebenfalls mitgelieferte metadata.csv-Datei stammt von https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python-Bibliotheken für die folgenden Analysen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verwendete Python-Bibliotheken\n",
    "\n",
    "* nltk\n",
    "  * \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "  * siehe https://www.nltk.org/\n",
    "* matplotlib\n",
    "  * \"Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\"\n",
    "  * siehe https://matplotlib.org/\n",
    "* wordcloud\n",
    "  * Dient zur Erzeugung von Wortwolken\n",
    "  * siehe https://pypi.org/project/wordcloud/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import-Möglichkeiten in Python \n",
    "\n",
    "(siehe z. B. https://www.digitalocean.com/community/tutorials/how-to-import-modules-in-python-3):\n",
    "\n",
    "1. Import eines gesamten Moduls\n",
    "```python\n",
    "import nltk\n",
    "```\n",
    "Allen Funktionen, die im Paket existieren, muss beim Aufruf der Paketname vorangestellt werden, z. B.\n",
    "```python\n",
    "nltk.download()\n",
    "```\n",
    "2. Import eines Moduls bzw. einer Funktion in einem Modul inklusive Umbenennung\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "Der vorangestellte Name beim Aufruf von Paketen kann so selbst gewählt werden:\n",
    "```python\n",
    "plt.figure(figsize=())\n",
    "```\n",
    "3. Import einzelner Funktionen direkt aus dem Modul\n",
    "```python\n",
    "from nltk.text import Text\n",
    "```\n",
    "Die Funktion kann ohne Präfix direkt aufgerufen werden:\n",
    "```python\n",
    "Text()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenaufbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import der NLTK-Bibliothek, die für alle folgenden Textbearbeitungsschritte benötigt wird.\n",
    "\n",
    "Vorlage: ```import nltk```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisierung\n",
    "\n",
    "#### NLTK-Tokenizer-Modell herunterladen\n",
    "Benötigt wird im Reiter Models das Tokenizer-Paket \"punkt\". \n",
    "\n",
    "<figure>\n",
    "    <img display:inline-block align=\"left\" src=\"Jupyter_Notebook_Screenshots/NLTK-Downloader.png\">\n",
    "    <br clear=\"all\" />\n",
    "</figure>\n",
    "\n",
    "Vorlage: `nltk.download()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"This tokenizer divides a text into a list of sentences\n",
    "by using an unsupervised algorithm to build a model for abbreviation\n",
    "words, collocations, and words that start sentences.  It must be\n",
    "trained on a large collection of plaintext in the target language\n",
    "before it can be used.\n",
    "\n",
    "The NLTK data package includes a pre-trained Punkt tokenizer for\n",
    "English.\n",
    "\n",
    "Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.  And sometimes sentences can start with non-capitalized words. i is a good variable name.\"\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/tokenize/punkt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Öffne die Datei 'abstracts_cord_19_random.txt' als file-Objekt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch den Befehl `open` wird die lokale Datei `abstracts_cord_19_random.txt` in eine Variable `file` vom Typ `TextIOWrapper` geschrieben. Der Parameter `'r'` gibt an, dass die Datei im Lesemodus geöffnet wird und die Angabe der Zeichenkodierung (`UTF-8`) stellt sicher, dass Sonderzeichen richtig dargestellt werden.\n",
    "\n",
    "Vorlage: `file = open('abstracts_cord_19_random.txt', 'r', encoding = 'UTF-8')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf Objekte der Klasse TextIOWrapper können verschiedene Methoden angewandt werden, beispielsweise ```read(num)```, um eine bestimmte Anzahl ```num``` Buchstaben aus dem Objekt in einen String umzuwandeln. Wird ```num``` weggelassen, wird die gesamte Datei als String ausgegeben. \n",
    "\n",
    "(Für weitere Methoden der Klasse TextIOWrapper siehe z. B. https://overiq.com/python-101/file-handling-in-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Wandle das File-Objekt in einen String um**\n",
    "\n",
    "Anwendung der read-Methode auf das file-Objekt und Schreiben des Rückgabewerts der Methode in die Variable abstracts_cord_raw\n",
    "\n",
    "Vorlage: `abstracts_cord_raw = file.read()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgeben der ersten 1000 Zeichen des Inhalts der Variable abstracts_cord_raw. (Bei der Ausgabe des kompletten Inhalts kann es sein, dass sich das Jupyter-Notebook aufhängt. Wenn der komplette Inhalte wiedergegeben werden soll, lautet der Befehl einfach `abstracts_cord_raw`, ohne eckige Klammern).\n",
    "\n",
    "Vorlage: `abstracts_cord_raw[0:1000]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Übung: Bitte geben Sie die nächsten 1000 Zeichen aus.*\n",
    "\n",
    "<p>\n",
    "<details>\n",
    "    Vorlage: <code>abstracts_cord_raw[1000:2000]</code>\n",
    "</details>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Eigentlicher Tokenisierungsschritt**\n",
    "\n",
    "Zerlegung des Abstract-Strings in einzelne Begriffe / Tokens. Dazu muss zunächst das ```word_tokenize``` Modul des NLTK importiert werden.\n",
    "\n",
    "Vorlage: ```from nltk.tokenize import word_tokenize```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Übergibt man der ```word_tokenize```-Funktion einen String, liefert die Funktion eine Liste der einzelnen Tokens, aus denen der String aufgebaut ist, zurück. (Die Funktion liefert dabei einzelne Wörter und Satzzeichen zurück, ist aber auf den ```Punkt```-Tokenizer angewiesen, der zunächst eine Zerlegung des Strings in einzelne Sätze durchführt, siehe https://www.nltk.org/_modules/nltk/tokenize.html).\n",
    "\n",
    "Vorlage: `cord_tokens = word_tokenize(abstracts_cord_raw)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe der ersten 20 Tokens\n",
    "\n",
    "Vorlage: `cord_tokens[0:20]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisierung (Umwandlung aller Tokens in Kleinschreibung)\n",
    "\n",
    "Ziel: alle Tokens klein schreiben, um Wörter, die sich nur in der Groß-/Kleinschreibung unterscheiden, zusammenfassen zu können.\n",
    "\n",
    "Die Funktion ```lower()``` wandelt einen beliebigen String in Kleinbuchstaben um. Um diese Funktion auf alle Tokens in der Liste oben anzuwenden, kann eine List Comprehension eingesetzt werden:\n",
    "\n",
    "```python\n",
    "lower_cord_tokens = [word.lower() for word in cord_tokens]\n",
    "```\n",
    "\n",
    "Gelesen werden kann diese als:\n",
    "\n",
    "lower_cord_tokens ist die Menge aller kleingeschriebenen Tokens \"word\", wobei \"word\" ein Element aus der Liste cord_tokens ist. \n",
    "\n",
    "Vorlage: `lower_cord_tokens = [word.lower() for word in cord_tokens]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe der ersten 20 kleingeschriebenen Tokens zur Überprüfung\n",
    "\n",
    "Vorlage: `lower_cord_tokens[0:20]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konkordanzliste\n",
    "\n",
    "Im Folgenden sollen die Konkordanzen eines bestimmten Tokens angezeigt werden. Konkordanzen sind die Kontexte, in denen ein bestimmtes Token vorkommt. Dazu kann die ```Text```-Klasse im ```text```-Paket des NLTK verwendet werden. Werden dieser Klasse die soeben erzeugten ```lower_cord_tokens``` zugeordnet, können mit der ```condordance```-Methode dieser Klasse die Kontexte jedes beliebigen Tokens ausgegeben werden.\n",
    "\n",
    "Zunächst muss aus dem NLTK-```text```-Paket die ```Text```-Klasse importiert werden:\n",
    "\n",
    "Vorlage: ```from nltk.text import Text```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend kann eine Instanz ```t``` der Klasse ```Text``` angelegt werden, wobei als Argument die Liste ```lower_cord_tokens``` übergeben wird.\n",
    "\n",
    "Vorlage: ``` t = Text(lower_cord_tokens)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Methode ```concordance``` können daraufhin die Kontexte eines frei wählbaren Tokens ausgegeben werden. Mit dem Parameter ```width``` kann angegeben werden, wie groß das Kontextfenster um den Suchbegriff sein soll, und mit ```lines```, wie viele Ergebniszeilen ausgegeben werden sollen.\n",
    "\n",
    "Vorlage: ```t.concordance('vaccine', width=80, lines=25)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Übung: Bitte geben Sie die Konkordanzen für ein selbst gewähltes Token aus (z. B. 'patient', 'virus', ...)*\n",
    "\n",
    "<p>\n",
    "<details>\n",
    "    Vorlage: <code>t.concordance('patient', width=80, lines=25)</code>\n",
    "</details>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Häufigkeitsverteilung\n",
    "\n",
    "Mit der NLTK-Funktion ```FreqDist``` (= Frequency Distribution) wird gezählt, wie oft jedes Token in einer Liste von Tokens auftaucht.\n",
    "\n",
    "1\\. Import der ```FreqDist-Funktion``` aus dem ```nltk.probability```-Modul\n",
    "\n",
    "Vorlage: `from nltk.probability import FreqDist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Schreiben der Häufigkeitsverteilung in eine Variable ```fdist```:\n",
    "\n",
    "Vorlage: `fdist = FreqDist(lower_cord_tokens)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```fdist``` hat die Form eines Dictionarys, mit den Tokens als Schlüssel und der jeweiligen Anzahl als Wert. Um das nachzuprüfen, kann der Inhalt von ```fdist``` ausgegeben werden.\n",
    "\n",
    "Vorlage: `fdist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Visualisierung der Häufigkeitsverteilung durch Anwendung der Methode ```plot()``` auf die Variable ```fdist```. \n",
    "\n",
    "Ausgegeben werden die häufigsten Token in absteigender Reihenfolge. Mit dem ersten Parameter, hier `30`, kann angegeben werden, wie viele Tokens angezeigt werden sollen. Mit dem Parameter `title` wird der Titel des Diagramms festgelegt (siehe https://www.nltk.org/api/nltk.probability.html?highlight=freqdist#nltk.probability.FreqDist.plot).\n",
    "\n",
    "Vorlage: `fdist.plot(30,title='Häufigkeitsverteilung der 30 häufigsten Token in der CORD-19-Abstractsammlung')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis: Man erkennt, dass die hier aufgeführten Token zum großen Teil nicht sinntragend sind (z. B. einzelne Zeichen wie Apostroph, Punkt und Doppelpunkt, daneben nicht-sinntragende Wörter wie Artikel oder Konjunktionen). Um sinnvolle Ergebnisse zu erhalten, sollten diese sogenannten \"Stoppworte\" entfernt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Häufigkeitsverteilung nach Stoppwortentfernung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download einer vorgefertigten NLTK-Stoppwortliste\n",
    "\n",
    "Vorlage: `nltk.download('stopwords')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heruntergeladene Stoppwörter importieren\n",
    "\n",
    "Vorlage: `from nltk.corpus import stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu sehen, welche Stoppworte in der Liste vorhanden sind, können diese ausgegeben werden (siehe https://www.geeksforgeeks.org/removing-stop-words-nltk-python/).\n",
    "\n",
    "Vorlage: `stopwords.words(\"english\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusätzlich ist es sinnvoll, auch Satz- und Sonderzeichen und einzelne Ziffern zu entfernen. Das ```String```-Modul enhält die beiden Strings ```string.punctuation``` und ```string.digits``` (siehe https://thomas-cokelaer.info/tutorials/python/module_string.html), die dazu genutzt werden können. Um auf die beiden Strings zugreifen zu können, muss zunächst das ```string```-Modul importiert werden.\n",
    "\n",
    "Vorlage: ```import string```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend können die beiden Strings testweise ausgegeben werden.\n",
    "\n",
    "1\\. Ausgabe von ```string.punctuation```\n",
    "\n",
    "Vorlage: ```string.punctuation```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Ausgabe von ```string.digits```\n",
    "\n",
    "Vorlage: ```string.digits```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ```string.punctuation``` und ```string.digits``` mit den Stoppworten in ```stopwords.words(\"english\")``` zu kombinieren, müssen ```string.punctuation``` und ```string.digits``` zuerst in den Datentyp ```list``` umgewandelt werden. Anschließend wird aus der kombinierten Liste ein sog. ```set``` erzeugt. Ein Set unterscheidet sich von einer Liste u. a. darin, dass jedes Element nur ein einziges Mal vorkommt und die Elemente ungeordnet vorliegen.\n",
    "\n",
    "Im Folgenden wird ein Set ```remove_these``` mit allen Stoppwörtern, Satzzeichen und Ziffern erzeugt. Die Prüfung, ob ein Element in einem Set vorhanden ist, ist schneller als die Prüfung, ob ein Element in einer Liste vorhanden ist (siehe z. B. https://www.oreilly.com/library/view/high-performance-python/9781449361747/ch04.html).\n",
    "\n",
    "Vorlage: ```remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inhalt von ```remove_these``` ausgeben:\n",
    "\n",
    "Vorlage: ```remove_these```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Stoppworte aus der Liste ```lower_cord_tokens``` entfernen. Dazu wird erneut eine List comprehension verwendet:\n",
    "\n",
    "```python\n",
    "filtered_text = [w for w in lower_cord_tokens if not w in remove_these]\n",
    "```\n",
    "\n",
    "Diese kann übersetzt werden in:\n",
    "\n",
    "Schreibe alle Tokens w in die Liste ```filtered_text```, die in der Liste ```lower_cord_tokens``` vorkommen, sofern sie nicht im Set ```remove_these``` enthalten sind.\n",
    "\n",
    "oder \n",
    "\n",
    "Die Menge ```filtered_text``` enthält alle Tokens w, die in der Menge ```lower_cord_tokens``` vorkommen und nicht im Set ```remove_these``` enthalten sind.\n",
    "\n",
    "Vorlage: `filtered_text = [w for w in lower_cord_tokens if not w in remove_these]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnung einer neuen Häufigkeitsverteilung auf der Grundlage der neuen Tokenliste ```filtered_text``` ohne Stoppwörter\n",
    "\n",
    "Vorlage: `fdist_filtered = FreqDist(filtered_text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisierung der 30 häufigsten Token nach Entfernung der Stoppwörter\n",
    "\n",
    "Vorlage: `fdist_filtered.plot(30, title = 'Frequency distribution for 30 most common tokens in the CORD-19 abstracts (excluding stopwords and punctuation)')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud\n",
    "\n",
    "Erzeugung einer Wortwolke mittels des wordcloud-Packages\n",
    "\n",
    "Vorlage: \n",
    "\n",
    "`import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud`\n",
    "\n",
    "\n",
    "`cloud = WordCloud(colormap=\"hsv\", width=1920,height=1080).generate_from_frequencies(fdist_filtered)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kollokationen\n",
    "\n",
    "Es könnte interessant sein, zu erfahren, welche Begriffe häufig zusammen vorkommen. Um diese Begriffe zu finden, kann nach Kollokationen gesucht werden, d. h. zwei Wörter, die im Text häufiger zusammen auftauchen, als durch Zufall erklärt werden kann. \n",
    "\n",
    "Für die Suche nach Kollokationen stellt das NLTK das Modul ```nltk.collocations``` zur Verfügung und darin die Funktionen ```BigramCollocationFinder``` und ```BigramAssocMeasures()```. Diese müssen zunächst importiert werden:\n",
    "\n",
    "Vorlage: \n",
    "```from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion ```BigramCollocationFinder``` erzeugt zwei Häufigkeitsverteilungen, eine für jedes Wort und eine andere für Bigramme. [6] Das erste Argument enhält die zu untersuchende Wortliste, in diesem Fall ```filtered_text```, d. h. den um Stoppworte bereinigten und kleingeschriebenen Text. Das zweite Argument, in diesem Fall ```5```, erlaubt ein Fenster von 5 Wörtern zwischen kollokierten Wörtern.\n",
    "\n",
    "Vorlage: ```finder = BigramCollocationFinder.from_words(filtered_text, 5)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu verhindern, dass selten auftretende Bigramme mit berücksichtigt werden, wird mit der Methode ```apply_freq_filter(10)``` eingestellt, dass Bigramme nur dann berücksichtigt werden sollen, wenn sie mindestens 10-mal vorkommen.\n",
    "\n",
    "Vorlage: ```finder.apply_freq_filter(10)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die gefundenen Bigramme zu bewerten, stehen über die Klasse ```BigramAssocMeasures``` verschiedene Bewertungsfunktionen zur Verfügung. Zunächst wird eine Instanz der Klasse ```BigramAssocMeasures()``` angelegt. \n",
    "\n",
    "Vorlage: ```bigram_measures = BigramAssocMeasures()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden mit der Methode ```likelihood_ratio``` die Top ```10``` Bigramme ermittelt.\n",
    "\n",
    "Vorlage: ```finder.nbest(bigram_measures.likelihood_ratio, 10)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausblick\n",
    "\n",
    "Das Natural Language Toolkit bietet eine Vielzahl weiterer Funktionen, die über die gezeigten einführenden Beispiele hinausgehen, wie beispielsweise unter https://www.oreilly.com/library/view/natural-language-processing/9780596803346/ zusammengefasst wird:\n",
    "\n",
    "* Extract information from unstructured text, either to guess the topic or identify \"named entities\"\n",
    "* Analyze linguistic structure in text, including parsing and semantic analysis\n",
    "* Access popular linguistic databases, including WordNet and treebanks\n",
    "* Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence\n",
    "\n",
    "(Die Inhalte des verlinkten Buchs müssten denen der freien Version unter https://www.nltk.org/book/ entsprechen.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vielen Dank für die Aufmerksamkeit!\n",
    "\n",
    "Haben Sie Fragen?\n",
    "\n",
    "Falls im Nachgang des Workshops noch Fragen aufkommen sollten, können Sie sich gerne jederzeit an uns wenden (tdm@ulb.tu-darmstadt.de)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
