{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214405f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T12:02:08.195308Z",
     "iopub.status.busy": "2022-11-16T12:02:08.194316Z",
     "iopub.status.idle": "2022-11-16T12:02:37.977799Z",
     "shell.execute_reply": "2022-11-16T12:02:37.974903Z"
    },
    "papermill": {
     "duration": 29.796489,
     "end_time": "2022-11-16T12:02:37.983787",
     "exception": false,
     "start_time": "2022-11-16T12:02:08.187298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925db68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T12:02:38.011787Z",
     "iopub.status.busy": "2022-11-16T12:02:38.009790Z",
     "iopub.status.idle": "2022-11-16T12:02:38.116961Z",
     "shell.execute_reply": "2022-11-16T12:02:38.114096Z"
    },
    "papermill": {
     "duration": 0.127121,
     "end_time": "2022-11-16T12:02:38.122957",
     "exception": false,
     "start_time": "2022-11-16T12:02:37.995836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source = pd.read_csv(\"data/langmeta.csv\").fillna(value = \"\")\n",
    "stop_words_german = []\n",
    "with open(\"data/stop_words_german.txt\") as file:\n",
    "    for line in file:\n",
    "        stop_words_german.append(line.replace('\\n', '').encode('ASCII','ignore').decode('ASCII'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05186b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T12:02:38.136955Z",
     "iopub.status.busy": "2022-11-16T12:02:38.135956Z",
     "iopub.status.idle": "2022-11-16T12:02:38.178555Z",
     "shell.execute_reply": "2022-11-16T12:02:38.176671Z"
    },
    "papermill": {
     "duration": 0.053594,
     "end_time": "2022-11-16T12:02:38.181549",
     "exception": false,
     "start_time": "2022-11-16T12:02:38.127955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (os.path.exists(\"data/keywordsmeta.csv\")):\n",
    "    os.remove(\"data/keywordsmeta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7cc3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T12:02:38.197582Z",
     "iopub.status.busy": "2022-11-16T12:02:38.196557Z",
     "iopub.status.idle": "2022-11-16T12:02:38.209553Z",
     "shell.execute_reply": "2022-11-16T12:02:38.207585Z"
    },
    "papermill": {
     "duration": 0.026039,
     "end_time": "2022-11-16T12:02:38.212585",
     "exception": false,
     "start_time": "2022-11-16T12:02:38.186546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "german_vec = CountVectorizer(\n",
    "    ngram_range=(1, 1), \n",
    "    stop_words=stop_words_german,\n",
    "    max_features=5,\n",
    "    strip_accents='ascii'\n",
    ")\n",
    "english_vec = CountVectorizer(\n",
    "    ngram_range=(1, 1), \n",
    "    stop_words=\"english\",\n",
    "    max_features=5,\n",
    "    strip_accents='ascii'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749605c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T12:02:38.226549Z",
     "iopub.status.busy": "2022-11-16T12:02:38.225555Z",
     "iopub.status.idle": "2022-11-16T12:25:01.980461Z",
     "shell.execute_reply": "2022-11-16T12:25:01.979529Z"
    },
    "papermill": {
     "duration": 1343.765946,
     "end_time": "2022-11-16T12:25:01.983493",
     "exception": false,
     "start_time": "2022-11-16T12:02:38.217547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "text=[]\n",
    "for index, row in source.iterrows():  \n",
    "    metadict = {\"Identifier\":row[\"Identifier\"]}      \n",
    "    if(row['Language'] != \"\"):\n",
    "        if(int(row['Textsize']) > 100):\n",
    "            if(row['Language'] == \"GERMAN\"):\n",
    "                textpfad = \"temp/\" + row[\"Identifier\"] + \".txt\"\n",
    "                headertextpfad = \"temp/\" + row[\"Identifier\"] + \"_header.txt\"\n",
    "                with open(textpfad, 'r', encoding=\"utf-8\") as textfile:\n",
    "                    german_vec.fit_transform(textfile)\n",
    "                    metadict.update({\"Topwords\":str(german_vec.get_feature_names_out())}) \n",
    "                try:\n",
    "                    with open(headertextpfad, 'r', encoding=\"utf-8\") as textfile:\n",
    "                        german_vec.fit_transform(textfile)\n",
    "                        metadict.update({\"Header_Topwords\":str(german_vec.get_feature_names_out())})\n",
    "                except:\n",
    "                    print(\"Error with Headertext\")\n",
    "            elif(row['Language'] == \"ENGLISH\"):\n",
    "                textpfad = \"temp/\" + row[\"Identifier\"] + \".txt\"\n",
    "                with open(textpfad, 'r', encoding=\"utf-8\") as textfile:\n",
    "                    english_vec.fit_transform(textfile)                    \n",
    "                    metadict.update({\"Topwords\":str(english_vec.get_feature_names_out())}) \n",
    "                try:\n",
    "                    with open(headertextpfad, 'r', encoding=\"utf-8\") as textfile:\n",
    "                        english_vec.fit_transform(textfile)                    \n",
    "                        metadict.update({\"Header_Topwords\":str(english_vec.get_feature_names_out())}) \n",
    "                except:\n",
    "                    print(\"Error with Headertext\")\n",
    "            else:\n",
    "                frame = pd.DataFrame(metadict, [0])\n",
    "                if (os.path.exists(\"data/keywordsmeta.csv\")):   \n",
    "                    newframe = pd.concat([oldframe, frame])\n",
    "                    newframe.to_csv(\"data/keywordsmeta.csv\", index=False)\n",
    "                    oldframe = newframe\n",
    "                else:\n",
    "                    frame.to_csv(\"data/keywordsmeta.csv\", index=False)\n",
    "                    oldframe = frame\n",
    "                metadict.clear\n",
    "                topwords=\"\"\n",
    "                print(\"finished: \", row[\"Identifier\"], \" couldn't match language\")\n",
    "                continue\n",
    "         #ask lobid.org to find a fitting keyword \n",
    "            if(\"Header_Topwords\" in metadict):\n",
    "                wordcloud = str(metadict[\"Header_Topwords\"]).replace(\"'\", \"\")\\\n",
    "                                                            .removeprefix(\"[\")\\\n",
    "                                                            .removesuffix(\"]\")\\\n",
    "                                                            .split(\" \")\n",
    "            else:\n",
    "                wordcloud = str(metadict[\"Topwords\"]).replace(\"'\", \"\")\\\n",
    "                                                     .removeprefix(\"[\")\\\n",
    "                                                     .removesuffix(\"]\")\\\n",
    "                                                     .split(\" \")\n",
    "            Keywordlist= list()\n",
    "            KeyIDlist =list()\n",
    "            for word in wordcloud:\n",
    "                api_url = \"https://lobid.org/gnd/search?q=preferredName:\" + word + \"&filter=type:SubjectHeading&format=json\"\n",
    "                response = requests.get(api_url)\n",
    "                if(response.status_code == requests.codes.ok):\n",
    "                    for member in response.json()[\"member\"]: \n",
    "                        if('SubjectHeading' in member[\"type\"] or 'SubjectHeadingSensoStricto' in member[\"type\"]):\n",
    "                            Keywordlist.append(member[\"preferredName\"])\n",
    "                            KeyIDlist.append(member[\"id\"])\n",
    "                            break\n",
    "\n",
    "            metadict.update({\"Keywordnames\":str(Keywordlist)})       \n",
    "            metadict.update({\"KeywordIDs\":str(KeyIDlist)})                           \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    frame = pd.DataFrame(metadict, [0])\n",
    "    if (os.path.exists(\"data/keywordsmeta.csv\")):   \n",
    "    #oldframe = pd.read_csv(\"data/langmeta.csv\")\n",
    "        newframe = pd.concat([oldframe, frame])\n",
    "        newframe.to_csv(\"data/keywordsmeta.csv\", index=False)\n",
    "        oldframe = newframe\n",
    "    else:\n",
    "        frame.to_csv(\"data/keywordsmeta.csv\", index=False)\n",
    "        oldframe = frame\n",
    "    metadict.clear\n",
    "    topwords=\"\" \n",
    "    print(\"finished: \", row[\"Identifier\"])\n",
    "    #break     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1384.044207,
   "end_time": "2022-11-16T12:25:06.767977",
   "environment_variables": {},
   "exception": null,
   "input_path": "C:\\Users\\Johann\\Desktop\\UNI\\BA\\pipenv\\notebooks\\keywordaggregation.ipynb",
   "output_path": "C:\\Users\\Johann\\Desktop\\UNI\\BA\\pipenv\\notebooks\\keywordaggregation.ipynb",
   "parameters": {},
   "start_time": "2022-11-16T12:02:02.723770",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
